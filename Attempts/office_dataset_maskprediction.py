# -*- coding: utf-8 -*-
"""Office Dataset-MaskPrediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jb8XVSo--qSVNoJ3MWRR6AoB9bDn7SSo

**Hardware Properties**
"""

!pip install --upgrade torch torchvision

import torch
from IPython.display import Image, clear_output 
from torch.utils.tensorboard import SummaryWriter
from time import time
print('PyTorch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))

"""**Initialize Tensorboard Object**"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

writer = SummaryWriter()

"""**Mount the Drive**"""

from google.colab import drive
drive.mount('/content/drive')
# how_long(start_time)

"""**Unzip the File**"""

import zipfile

def zip_data(zipfile, path_to_zipfile, directory_to_extract):
  print('Outside loop')
  with zipfile.ZipFile(path_to_zipfile, 'r') as zip:
    print('Extracting ZIP')
    zip.extractall(directory_to_extract)
  print("Zipfile extracted to", directory_to_extract)

path = '/content/drive/My Drive/ZipFiles/data_TenK101.zip'
save = '/content/drive/My Drive/Videos'
zip_data(zipfile,path, save)

# zip = zipfile.ZipFile('/content/drive/My Drive/ZipFiles/data_TenK101.zip')
# zip.extractall(save)

import os
os.listdir('/content/drive/My Drive/ZipExtract')

"""**Set the path for all 4 type of Images**"""

import os
zip_path = "/content/drive/My Drive/ZipExtract/data_TenK101"
subd = os.listdir(zip_path) #Returns sub-directories

bg_path = '/content/drive/My Drive/ZipExtract/data_TenK101/BG'

bgfg_path = '/content/drive/My Drive/ZipExtract/data_TenK101/BG_FG' 

bgfg_mask_path = '/content/drive/My Drive/ZipExtract/data_TenK101/BG_FG Masks'

depthmap_path = '/content/drive/My Drive/ZipExtract/data_TenK101/DepthMaps'

sorted(os.listdir(bgfg_mask_path))[:-10]

from PIL import Image
import cv2
img = cv2.imread('/content/drive/My Drive/ZipExtract/data_TenK101/BG_FG Masks/bg_001_fg_001_001.jpeg')
img.shape

foldersize=[len(os.listdir(path)) for path in [bg_path, bgfg_path, bgfg_mask_path, depthmap_path]]
foldersize

mask = sorted([ file for file in os.listdir(bgfg_mask_path)])
mask[:4]

"""**Image Segmentation File**"""

# NOTE: Have done a lot of commenting below, because it's tricky.

# Now the first task is to clip input and output
# This is done since BG has only 100 images while BG_FG has around 400K, but for this I'm using 10K
# Now to do this I've used this snippet below, which will clip in the inputs with it's corresponding ground truth

masks = [mask for mask in os.listdir(bgfg_mask_path)] # Mask Files

bg_idx = [int(m.split('_')[1]) for m in masks] # Background file index
# set(bg_idx)

bgfiles = [os.path.join(bg_path, bg) for bg in os.listdir(bg_path)] # all 100 BG Files
rev_bg_files = sorted([os.path.join(bgfg_mask_path, bgfiles[idx-1]) for idx in bg_idx ]) # BG Files of Proper index
rev_bg_files

"""**Zipping Data**"""

# bg_files = [os.path.join(bg_path, bg) for bg in os.listdir(bg_path)] # BG Files
# start_time = t.time()
# I have zipped both bg and bg_fg as image segmentation input, this is obvious. I don't think I need to explain here

bgfg_mask_files = sorted([os.path.join(bgfg_mask_path, mask) for mask in os.listdir(bgfg_mask_path)]) # Mask Files
bgfg_files = sorted([os.path.join(bgfg_path, bgfg) for bgfg in os.listdir(bgfg_path)]) # bgfg files
dpmap_files = sorted([os.path.join(depthmap_path, map) for map in os.listdir(depthmap_path)]) #Depth Maps

img_seg = list(zip(rev_bg_files, bgfg_files, bgfg_mask_files, dpmap_files)) # for bg add rev_bg_files directly and for depth estimation you can add depth maps
type(img_seg)

img_seg # this is the final data for image segmentations, go and use it

"""**Custom Depth Map Dataset Class**"""

from torch.utils.data import Dataset, random_split
from PIL import Image
import numpy as np
import torch
import os
import torchvision.transforms as transforms
from tqdm import notebook
import zipfile


class DepthMapDataset(Dataset):
  
  # Image Segmentation Part
  def __init__(self, data, bg_transforms, bgfg_transforms, mask_transforms, depthmap_transforms): 
    self.bg_images, self.bgfg_images, self.mask_images, self.depth_maps = zip(*data) # do the unzipping part, add this during depth estimation self.depth_maps, self.bg_images,
    self.bg_transforms = bg_transforms
    self.bgfg_transforms = bgfg_transforms
    self.mask_transforms = mask_transforms
    self.depthmap_transforms = depthmap_transforms


  def __len__(self):
    """
    returns len of the dataset
    """
    return len(self.bgfg_images)


  def __getitem__(self, idx):
    """
    returns image data & target for the corresponding index
    """
    try:
      bg_image = Image.open(self.bg_images[idx]).convert('L') # BG Images
      
      bgfg_image = Image.open(self.bgfg_images[idx]).convert('L') # BG_FG Images  
      
      mask_image = Image.open(self.mask_images[idx]).convert('L') # Mask Images

      depth_map = Image.open(self.depth_maps[idx]).convert('L') # Depth Images

      ## Transformed Images
      bg_image = self.bg_transforms(bg_image)
      bgfg_image = self.bgfg_transforms(bgfg_image)
      mask_image = self.mask_transforms(mask_image) # first transform the image
      depth_map = self.depthmap_transforms(depth_map)

      # upd_mask = mask_image.unsqueeze(0) # then add one channel by unsqueezing it
      # print('shape of mask_image', upd_mask.size())

      return {"bg": bg_image, "bgfg": bgfg_image, "mask": mask_image, "dpmap": depth_map} # dict way of returning Depth Map
      # also add "bg": bg_image,
    

    except Exception as e:
      print("Image {0} skipped due to {1}".format(self.bgfg_images[idx],e)) # this is only if some images can't be identified by PIL

"""**ALBUMENTATIONS**"""

from torchvision import transforms
import albumentations as A
import albumentations.pytorch as AP
import random
import numpy as np

class AlbumentationTransforms:
  """
  Helper class to create test and train transforms using Albumentations
  """
  def __init__(self, transforms_list=[]):
    transforms_list.append(AP.ToTensor()) # Transform the normalized image to Tensor
    self.transforms = A.Compose(transforms_list)
    print('transforms list', transforms_list)


  def __call__(self, img):
    img = np.array(img) 
    # a = self.transforms(image=img)['image']
    # # print('type of transf a', type(a))
    return self.transforms(image=img)['image']

  

class SingleChannel_ImageTransforms:
  """
  Helper class to create test and train transforms using Albumentations
  """
  def __init__(self, transforms_list=[]):
    transforms_list.append(AP.ToTensor()) # Transform the normalized image to Tensor
    self.transforms = A.Compose(transforms_list)
    print('transforms list', transforms_list)


  def __call__(self, img):
    img = np.array(img)
    img = img[:,:,np.newaxis] 
    # a = self.transforms(image=img)['image']
    # # print('type of transf a', type(a))
    return self.transforms(image=img)['image']

"""**Image Transformations**"""

import albumentations as A
import albumentations.pytorch as AP

# BG Images
bg_mean = (0.5411)
bg_stdev = (0.2320)

# FG_BG Images
fgbg_mean = (0.513)#, 0.536, 0.5546)
fgbg_stdev = (0.2355)#, 0.2317, 0.2326)

# FG Mask Images   # SINGLE CHANNEL
mask_mean = (0.05207)
mask_stdev = (0.21686)#, 0.4025, 0.4025)

# Depth Maps
dpmap_mean = (0.3045)
dpmap_stdev = (0.1130)

# Used same type of transformations for both Train & Test Data

bg_transform = SingleChannel_ImageTransforms([A.Resize(192,192,p=1),
                                                A.Normalize(mean=bg_mean,
                                                std=bg_stdev)])

bgfg_transform = SingleChannel_ImageTransforms([A.Resize(192,192,p=1),
                                                # A.HorizontalFlip(p=0.5),
                                                A.IAAAdditiveGaussianNoise(p=0.2),
                                                A.Normalize(mean=fgbg_mean,
                                              std=fgbg_stdev)])

bgfg_mask_transform = SingleChannel_ImageTransforms([A.Resize(96,96,p=1),
                                                    A.Normalize(mean=mask_mean,
                                                    std=mask_stdev)])

dpmap_transform = SingleChannel_ImageTransforms([A.Resize(96,96,p=1),
                                                 A.Normalize(mean=dpmap_mean,
                                                std=dpmap_stdev)])


# dpmap_transforms = AlbumentationTransforms([A.Normalize(mean=dpmap_mean, 
                                              # std=dpmap_std)])
# train_dataset , test_dataset = DepthMapDataset(splitRatio = 70, test_transforms = test_transform,train_transforms = train_transform)

train = DepthMapDataset(img_seg[:7000], bg_transform,  bgfg_transform, bgfg_mask_transform, dpmap_transform) #bg_transform,, dpmap_transform)
test = DepthMapDataset(img_seg[7001:], bg_transform, bgfg_transform, bgfg_mask_transform, dpmap_transform)

# # a,b=zip(*img_seg)
# len(b)

a=train[3]
mask = a['mask']
mask_t = mask
mask = mask_t.cpu()
# type(bgfg_cpu)

print("mask size is: ", mask.size())
from torchvision import transforms
im = transforms.ToPILImage()(mask).convert("RGB")
display(im)
print(im)
print(im.size)

a=train[3]
mask = a['bgfg']
mask_t = mask
mask = mask_t.cpu()
# type(bgfg_cpu)

print("mask size is: ", mask.size())
from torchvision import transforms
im = transforms.ToPILImage()(mask).convert("RGB")
display(im)
print(im)
print(im.size)

import matplotlib.pyplot as plt


a=train[6]

bgfg= a['mask']
upd = bgfg.squeeze(0)
bgfg_np = upd/2+0.5
np_img = bgfg_np.numpy()

plt.imshow(np_img[:,:])

# print("bgfg size is: ", bgfg.size())
# from torchvision import transforms
# im = transforms.ToPILImage()(bgfg).convert("L")
# display(im)
# print(im)
# print(im.size)

a=train[9]
bgfg= a['mask']

# bgfg = bgfg.cpu()
import matplotlib.pyplot as plt

plt.imshow(  bgfg.permute(1, 2, 0)  )

a = np.ones((2,2))
b=np.expand_dims(a, 0)
b

import torch
import torchvision


class DataLoaders:
  def __init__(self, 
              batch_size=512,
              shuffle=True,
              num_workers=4,
              pin_memory=True,
              seed=1):
  
    """
    Arguments:-
    batch_size: Number of images to be passed in each batch
    shuffle(boolean):  If True, then shuffling of the dataset takes place
    num_workers(int): Number of processes that generate batches in parallel
    pin_memory(boolean):
    seed: Random Number, this is to maintain the consistency
    """
    use_cuda = torch.cuda.is_available()
    device = torch.device('cuda' if use_cuda else 'cpu')  # set device to cuda

    if use_cuda:
      torch.manual_seed(seed)
    
    self.dataLoader_args = dict(batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True ) if use_cuda else dict(batch_size=1, shuffle=True, num_workers = 1, pin_memory = True)


  def dataLoader(self, data):
    return torch.utils.data.DataLoader(data,**self.dataLoader_args)



def Data_To_Dataloader(trainset,testset,seed=1,batch_size=128,num_workers=4,pin_memory=True):
	"""
	Conv DataSet Obj to DataLoader
	"""

	SEED = 1

	# CUDA?
	cuda = torch.cuda.is_available()

	# For reproducibility
	torch.manual_seed(SEED)

	if cuda:
			torch.cuda.manual_seed(SEED)

	dataloader_args = dict(shuffle=True, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory) if cuda else dict(shuffle=True, batch_size=64)

	trainloader = torch.utils.data.DataLoader(trainset, **dataloader_args)
	testloader = torch.utils.data.DataLoader(testset, **dataloader_args)


	return  trainloader, testloader

"""**TRAINLOADER & TESTLOADER**"""

trainLoader, testLoader = Data_To_Dataloader(train,test,batch_size=48)

"""**Writing to Tensorboard**"""

import matplotlib.pyplot as plt
dataiter = iter(trainLoader)
bg, bgfg, mask, dpmap = next(dataiter)['bg'], next(dataiter)['bgfg'], next(dataiter)['mask'], next(dataiter)['dpmap']

bgfg_grid = torchvision.utils.make_grid(bgfg)

# matplotlib_imshow(bgfg_images)
writer.add_image('BG_FG Images', bgfg_grid)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard -- logdir=/content/runs/Aug05_10-08-37_c31f995905fa
# %tensorboard --logdir logs/tensorboard

"""**UNET IMPLEMENTATION**"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class BatchNorm(nn.BatchNorm2d):
    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight=True, bias=True):
        super().__init__(num_features, eps=eps, momentum=momentum)
        self.weight.data.fill_(1.0)
        self.bias.data.fill_(0.0)
        self.weight.requires_grad = weight
        self.bias.requires_grad = bias


class GhostBatchNorm(BatchNorm):
    def __init__(self, num_features, num_splits, **kw):
        super().__init__(num_features, **kw)
        self.num_splits = num_splits
        self.register_buffer('running_mean', torch.zeros(num_features * self.num_splits))
        self.register_buffer('running_var', torch.ones(num_features * self.num_splits))

    def train(self, mode=True):
        if (self.training is True) and (mode is False):  # lazily collate stats when we are going to use them
            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(
                self.num_splits)
            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(
                self.num_splits)
        return super().train(mode)

    def forward(self, input):
        N, C, H, W = input.shape
        if self.training or not self.track_running_stats:
            return F.batch_norm(
                input.view(-1, C * self.num_splits, H, W), self.running_mean, self.running_var,
                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),
                True, self.momentum, self.eps).view(N, C, H, W)
        else:
            return F.batch_norm(
                input, self.running_mean[:self.num_features], self.running_var[:self.num_features],
                self.weight, self.bias, False, self.momentum, self.eps)



class DoubleConv(nn.Module):
    """Double Convolution --->(Convolution => [BN] => ReLU) * 2"""

    def __init__(self, in_channels, out_channels, mid_channels=None, dropout_value=0.2, GBN=False):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(mid_channels),
            nn.Dropout(dropout_value),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.Dropout(dropout_value),
            nn.ReLU(inplace=True))
        
        if GBN:
          self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(mid_channels),
            nn.Dropout(dropout_value),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),
            GhostBatchNorm(out_channels, 1),
            nn.Dropout(dropout_value),
            nn.ReLU(inplace=True))



    def forward(self, x):
        return self.double_conv(x)


class DownSample(nn.Module):
    """Downsampling Channel Size with maxpool followed by Double Conv"""

    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_channels, out_channels)
        )

    def forward(self, x):
        return self.maxpool_conv(x)


class UpSample(nn.Module):
    """Upsampling followed by Double Convolution"""

    def __init__(self, in_channels, out_channels, bilinear=True):
        super().__init__()

        # if bilinear, use the normal convolutions to reduce the number of channels
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)
        else:
            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv(in_channels, out_channels)


    def forward(self, x1, x2):
        x1 = self.up(x1)
        # input is CHW
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]

        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
                        diffY // 2, diffY - diffY // 2])
        # if you have padding issues, see
        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)

# DEPTH

        # self.inc = DoubleConv(n_channels, 64)
        # self.down1 = DownSample(64, 128)
        # self.down2 = DownSample(128, 256)
        # self.down3 = DownSample(256, 512)
        # factor = 2 if bilinear else 1
        # self.down4 = DownSample(512, 1024 // factor)
        # self.up1 = UpSample(1024, 512 // factor, bilinear)
        # self.up2 = UpSample(512, 256 // factor, bilinear)
        # self.up3 = UpSample(256, 128 // factor, bilinear)
        # self.up4 = UpSample(128, 64, bilinear)
        # self.outc = OutConv(64, n_classes)

class UNet(nn.Module):
    def __init__(self, n_channels, n_classes, bilinear=True):
        super(UNet, self).__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear

        

         # -------------- Mask -------------------
        self.inc = DoubleConv(n_channels, 32, GBN=True)
        self.double_conv = DoubleConv(32, 32, GBN=True) 
        self.down1_mask = DownSample(32, 64)
        self.down2_mask = DownSample(64, 128)
        self.down3_mask = DownSample(128, 256)

        factor = 2 if bilinear else 1

        self.down4_mask = DownSample(256, 512 // factor)
        self.up1_mask = UpSample(512, 256 // factor, bilinear)
        self.up2_mask = UpSample(256, 128 // factor, bilinear)
        self.up3_mask = UpSample(128, 64 // factor, bilinear)
        self.up4_mask = UpSample(64, 32, bilinear)
        self.outc_mask = OutConv(32, n_classes)



    def forward(self, bgfg): # add background and crop & concatenate the same at right position
      # # BG
      # bg1 = self.bg_inc(bg)
      # print('bg1 shape', bg1.size())

      # bg2 = self.bg_downsample(bg1)
      # print('bg2 shape', bg2.size()) # 96x96x32



      x1 = self.inc(bgfg)        
      # print('x1 shape', x1.size())

      x2_mask = self.down1_mask(x1)
      # print('x2_mask shape', x2_mask.size())
      
      x3_mask = self.down2_mask(x2_mask)
      # print('x3_mask shape', x3_mask.size())
      
      x4_mask = self.down3_mask(x3_mask)
      # print('x4_mask shape', x4_mask.size())
      
      x5_mask = self.down4_mask(x4_mask)
      # print('x5_mask shape', x5_mask.size())
      
      x_mask = self.up1_mask(x5_mask, x4_mask)
      # print('x_mask1 shape', x_mask.size())
      
      x_mask = self.up2_mask(x_mask, x3_mask)
      # print('x_mask2 shape', x_mask.size())
      
      x_mask = self.up3_mask(x_mask, x2_mask)
      # print('x_mask3 shape', x_mask.size()) # 96x96x32
      
      # x_mask = self.up4_mask(x_mask, x1)
      # print('x_mask4 shape', x_mask.size())
      
      logits_mask = self.outc_mask(x_mask)
      # print('logits_mask shape', logits_mask.size())
  
      
      return logits_mask

from torchsummary import summary
import torch
cuda = torch.cuda.is_available()
device = torch.device("cuda" if cuda else "cpu")
unet_model = UNet(2,1).to(device)

summary(unet_model, input_size=(2, 192, 192))

"""**LOSS FUNCTIONS & CO-EFFICIENT**

**Jaccard Score**
"""

def f_score(prediction, ground_truth, beta=1, eps=1e-7, threshold=None, activation='sigmoid'):
    """
    Args:
        pr (torch.Tensor): A list of predicted elements
        gt (torch.Tensor):  A list of elements that are to be predicted
        eps (float): epsilon to avoid zero division
        threshold: threshold for outputs binarization
    Returns:
        float: IoU (Jaccard) score
    """
    if activation is None:
        activation_fn = lambda x: x
    elif activation == "sigmoid":
        activation_fn = torch.nn.Sigmoid()
    elif activation == "softmax2d":
        activation_fn = torch.nn.Softmax2d()
    else:
        raise NotImplementedError(
            "Activation implemented for sigmoid and softmax2d"
        )

    prediction = activation_fn(prediction)

    if threshold is not None:
        prediction = (prediction > threshold).float()


    tp = torch.sum(ground_truth * prediction) # TRUE POSITIVE
    fp = torch.sum(prediction) - tp  # FALSE POSITIVE
    fn = torch.sum(ground_truth) - tp  # FALSE NEGATIVE

    jaccard_score = ((1 + beta ** 2) * tp + eps) \
            / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)

    return jaccard_score

"""**DICE LOSS**"""

class Dice(nn.Module):
    __name__ = 'dice_loss'

    def __init__(self, eps=1e-7, activation='sigmoid'):
        super().__init__()
        self.activation = activation
        self.eps = eps

    def forward(self, y_pr, y_gt):
        return 1 - f_score(y_pr, y_gt, beta=1., 
                           eps=self.eps, threshold=None, 
                           activation=self.activation)


class BCEDiceLoss(Dice):
  
    def __init__(self, eps=1e-7, activation='sigmoid', lambda_dice=1.0, lambda_bce=1.0):
        super().__init__(eps, activation)
        if activation == None:
            self.bce = nn.BCELoss(reduction='mean')
        else:
            self.bce = nn.BCEWithLogitsLoss(reduction='mean')
        self.lambda_dice=lambda_dice
        self.lambda_bce=lambda_bce

    def forward(self, y_pr, y_gt):
        dice = super().forward(y_pr, y_gt)
        bce = self.bce(y_pr, y_gt)
        return (self.lambda_dice*dice) + (self.lambda_bce* bce)

#PyTorch
class DiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceLoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        
        #comment out if your model contains a sigmoid or equivalent activation layer
        inputs = F.sigmoid(inputs)       
        
        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        intersection = (inputs * targets).sum()                            
        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  
        
        return 1 - dice

#IOU Loss
#PyTorch
class IoULoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(IoULoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        
        #comment out if your model contains a sigmoid or equivalent activation layer
        inputs = F.sigmoid(inputs)       
        
        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        #intersection is equivalent to True Positive count
        #union is the mutually inclusive area of all labels & predictions 
        intersection = (inputs * targets).sum()
        total = (inputs + targets).sum()
        union = total - intersection 
        
        IoU = (intersection + smooth)/(union + smooth)
                
        return 1 - IoU

"""**SSIM LOSS**"""

import torch
import torch.nn.functional as F
from math import exp
import numpy as np


def gaussian(window_size, sigma):
    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])
    return gauss/gauss.sum()


def create_window(window_size, channel=1):
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()
    return window


def ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):
    # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).
    if val_range is None:
        if torch.max(img1) > 128:
            max_val = 255
        else:
            max_val = 1

        if torch.min(img1) < -0.5:
            min_val = -1
        else:
            min_val = 0
        L = max_val - min_val
    else:
        L = val_range

    padd = 0
    (_, channel, height, width) = img1.size()
    if window is None:
        real_size = min(window_size, height, width)
        window = create_window(real_size, channel=channel).to(img1.device)

    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)
    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)

    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2

    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq
    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq
    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2

    C1 = (0.01 * L) ** 2
    C2 = (0.03 * L) ** 2

    v1 = 2.0 * sigma12 + C2
    v2 = sigma1_sq + sigma2_sq + C2
    cs = torch.mean(v1 / v2)  # contrast sensitivity

    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)

    if size_average:
        ret = ssim_map.mean()
    else:
        ret = ssim_map.mean(1).mean(1).mean(1)

    if full:
        return ret, cs
    return ret


def msssim(img1, img2, window_size=11, size_average=True, val_range=None, normalize=None):
    device = img1.device
    weights = torch.FloatTensor([0.0448, 0.2856, 0.3001, 0.2363, 0.1333]).to(device)
    levels = weights.size()[0]
    ssims = []
    mcs = []
    for _ in range(levels):
        sim, cs = ssim(img1, img2, window_size=window_size, size_average=size_average, full=True, val_range=val_range)

        # Relu normalize (not compliant with original definition)
        if normalize == "relu":
            ssims.append(torch.relu(sim))
            mcs.append(torch.relu(cs))
        else:
            ssims.append(sim)
            mcs.append(cs)

        img1 = F.avg_pool2d(img1, (2, 2))
        img2 = F.avg_pool2d(img2, (2, 2))

    ssims = torch.stack(ssims)
    mcs = torch.stack(mcs)

    # Simple normalize (not compliant with original definition)
    # TODO: remove support for normalize == True (kept for backward support)
    if normalize == "simple" or normalize == True:
        ssims = (ssims + 1) / 2
        mcs = (mcs + 1) / 2

    pow1 = mcs ** weights
    pow2 = ssims ** weights

    # From Matlab implementation https://ece.uwaterloo.ca/~z70wang/research/iwssim/
    output = torch.prod(pow1[:-1] * pow2[-1])
    return output


# Classes to re-use window
class SSIM(torch.nn.Module):
    def __init__(self, window_size=11, size_average=True, val_range=None):
        super(SSIM, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.val_range = val_range

        # Assume 1 channel for SSIM
        self.channel = 1
        self.window = create_window(window_size)

    def forward(self, img1, img2):
        (_, channel, _, _) = img1.size()

        if channel == self.channel and self.window.dtype == img1.dtype:
            window = self.window
        else:
            window = create_window(self.window_size, channel).to(img1.device).type(img1.dtype)
            self.window = window
            self.channel = channel

        return ssim(img1, img2, window=window, window_size=self.window_size, size_average=self.size_average)

class MSSSIM(torch.nn.Module):
    def __init__(self, window_size=11, size_average=True, channel=3):
        super(MSSSIM, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.channel = channel

    def forward(self, img1, img2):
        # TODO: store window between calls if possible
        return msssim(img1, img2, window_size=self.window_size, size_average=self.size_average)

import torch
import torchvision
from tqdm import notebook
import torch.nn.functional as F
# from time import time

unet_model = UNet(2,2).to(device)

# CRITERION PART
mask_criterion = BCEDiceLoss()
depth_criterion = IoULoss()
# mask_criterion = nn.BCEWitthLogitsLoss()

optimizer = torch.optim.Adam(unet_model.parameters(), lr = 0.01)
current_lr = [param_group['lr'] for param_group in optimizer.param_groups][0]
print(current_lr)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2, threshold=0.001)

# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma = 0.4)

import time

epochs = 40 # setting epochs
train_losses, valid_losses, train_dice, train_iou, test_dice, test_iou, lr_rates= [], [], [], [], [], [], [] # initialising lists to store model stats

# _input_samples = [] # to collect input images

_trainsamples = [] # to collect train samples
_testsamples = [] # to collect test samples

valid_loss_min = np.inf # to compare validation loss

m_ssim_index = [] # mask ssim indices
d_ssim_index = [] # depth ssim indices

for epoch in range(epochs+1):

###########################
  epoch_start = time.time()
###########################
  
  print('EPOCH:', epoch)
  train_loss, valid_loss, dice_score, iou_score, mask_ssim_index, depth_ssim_index = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 # initializing batch params

  #################
  # MODEL TRAINING#
  #################

  unet_model.train() 
  pbar = notebook.tqdm(enumerate(trainLoader))

  for batch_idx, data in enumerate(pbar):
    
    ########################
    batch_time = time.time()
    ########################

    # assigning bgfg and mask with a bgfg and mask image from trainloader
    bg, bgfg, mask, dpmap = data[1]['bg'].to(device), data[1]['bgfg'].to(device), data[1]['mask'].to(device), data[1]['dpmap'].to(device)
    
    # concatenate two input images
    _input = torch.cat((bg, bgfg), dim=1)
    # print('_input shape', _input.size())


  
    ###########################
    data_loadtime = time.time()
    # print(f'Took {data_loadtime-batch_time} seconds to load data.')
    ###########################

    # clear the gradients of all optimized variables
    optimizer.zero_grad()

    # Run the model on this input batch
    output = unet_model(_input) # 96x96x2
    
    # predictions
    pred_mask = output[:, :1, :, :] # N:C:H:W = 48,1,96,96
    pred_dpmap = output[:, 1:, :, :] # 48,1,96,96

    # calculate loss using criterion
    maskloss = mask_criterion(output[:, :1, :, :], mask) # Output, Target
    dpmaploss = depth_criterion(output[:, 1:, :, :], dpmap)


    # mask_ssim_index = ssim(output[:, :1, :, :], mask)
    # mask_ssim = torch.clamp((1 - ssim(output[:, :1, :, :], mask)) * 0.5, 0, 1)

    depth_ssim_index = ssim(output[:, 1:, :, :], dpmap)
    depth_ssim = torch.clamp((1 - ssim(output[:, 1:, :, :], dpmap)) * 0.5, 0, 1)

    
    loss = (depth_ssim *0.9 + dpmaploss *0.1) + maskloss 

    # for backpropagation
    loss.backward()

    # parameter update
    optimizer.step()

    # For checking input images
    if batch_idx == 1: #
      for i in range(10):
        _trainsamples.append({'pred_dpmap': pred_dpmap[i] , 'pred_mask': pred_mask[i],'dpmap': dpmap[i], 'mask': mask[i], 'bgfg': bgfg[i], 'bg': bg[i]})
# try adding output[] instead of pred 

    # calculate average training loss
    train_data_size = len(trainLoader)*trainLoader.batch_size
    train_loss += loss.item()*trainLoader.batch_size

    # mask_ssim_index += mask_ssim_index.item()*trainLoader.batch_size
    depth_ssim_index += depth_ssim_index.item()*trainLoader.batch_size

    dice_score += (1-maskloss.item())#*trainLoader.batch_size
    iou_score += (1-dpmaploss.item())*trainLoader.batch_size

    # Update PBAR-TQDM
    pbar.set_description(f'D_SSIM={depth_ssim_index.item():0.3f};  IoU={1-dpmaploss.item():0.3f}')
    

  # append train loss and update the same
  train_loss = train_loss/train_data_size
  depth_ssim_index = depth_ssim_index/train_data_size
  # dice_coeff = dice_score/test_data_size
  iou_coeff = iou_score/test_data_size


  train_dice.append(dice_coeff)
  train_iou.append(iou_coeff)
 
  train_losses.append(train_loss)
  d_ssim_index.append(depth_ssim_index)
  
 
  


  ###################
  # MODEL VALIDATION#
  ###################

  unet_model.eval()
  pbar = notebook.tqdm(enumerate(testLoader))
  validation = 0 

  # setting no_grad
  with torch.no_grad(): 
    for batch_idx, data in enumerate(pbar):
      bg, bgfg, mask, dpmap = data[1]['bg'].to(device), data[1]['bgfg'].to(device), data[1]['mask'].to(device), data[1]['dpmap'].to(device)

      # concatenate two input images
      _input = torch.cat((bg, bgfg), dim=1)
      # print('_input shape', _input.size())

      # Model Predictions
      output = unet_model(_input) 
      
      # predictions
      pred_mask = output[:, :1, :, :] # N:C:H:W
      pred_dpmap = output[:, 1:, :, :]

      

      # calculate loss using criterion
      maskloss = mask_criterion(output[:, :1, :, :], mask) # Output, Target
      dpmaploss = depth_criterion(output[:, 1:, :, :], dpmap)

      # mask_ssim_index = ssim(output[:, :1, :, :], mask)
      # mask_ssim = torch.clamp((1 - ssim(output[:, :1, :, :], mask)) * 0.5, 0, 1)

      depth_ssim_index = ssim(output[:, 1:, :, :], dpmap)
      depth_ssim = torch.clamp((1 - ssim(output[:, 1:, :, :], dpmap)) * 0.5, 0, 1)

      
      loss =  depth_ssim *0.9 + dpmaploss *0.1 + maskloss 

     
      # calculate average test loss
      test_data_size = len(testLoader)*testLoader.batch_size
      valid_loss += loss.item()*testLoader.batch_size
      depth_ssim_index += depth_ssim_index.item()*testLoader.batch_size
      # mask_ssim_index += mask_ssim_index.item()*testLoader.batch_size

      # Update PBAR-TQDM
      pbar.set_description(f'D_SSIM={depth_ssim_index.item():0.3f}; IoU={1-dpmaploss.item():0.3f}')


      # For checking prediction results
      if batch_idx == len(testLoader)-1: #
        for i in range(10):
          _testsamples.append({'pred_dpmap': pred_dpmap[i], 'pred_mask': pred_mask[i] ,'dpmap': dpmap[i], 'mask': mask[i], 'bgfg': bgfg[i], 'bg': bg[i]})

    #  current lr
    clr = [param_group['lr'] for param_group in optimizer.param_groups]
    print('Learning Rate:', clr)

    # update learning rate
    scheduler.step(valid_loss)

    # update loss and loss-coefficients
    valid_loss = valid_loss/test_data_size
    depth_ssim_index = depth_ssim_index/test_data_size

    # dice_coeff = dice_score/test_data_size
    iou_coeff = iou_score/test_data_size

    # append the same 
    valid_losses.append(valid_loss_min)
    d_ssim_index.append(depth_ssim_index)
    # test_dice.append(dice_coeff)
    test_iou.append(iou_coeff)
    lr_rates.append(clr)

   
    ############################
    batch_end_time = time.time()
    # print(f'Took {batch_end_[[time]] - batch_time} seconds to train {batch_idx} batch.')
    ############################

  

 # print training/validation statistics
  print('IoU_Score={:.3f} '.format(iou_coeff))


  #############################
  end_epoch_time = time.time()
  ############################
  # print(f'Data Loading for {epoch} took {end_dataload-epoch_start} seconds.')
  print(f'Epoch {epoch} took {end_epoch_time-epoch_start} seconds.')

  # save model if validation loss has decreased
  if valid_loss <= valid_loss_min:
    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))
    torch.save(unet_model.state_dict(), '/content/drive/My Drive/Saved S15 Models/image_segmentation.pt')
    valid_loss_min = valid_loss

import gc
gc.collect()

def plot_trainsamples(sampledata, title='Train Samples'):
  datalen = len(sampledata)
  for idx in range(len(sampledata)): #sampledata[idx]['pred_mask'],
    bgfg, mask, pred_mask,  pred_dpmap = sampledata[idx]['bgfg'], sampledata[idx]['mask'], sampledata[idx]['pred_mask'], sampledata[idx]['pred_dpmap'] # Tensors of BG,BGFG,MASK & DPMAP
    image_set = (bgfg, mask,pred_mask, pred_dpmap)
    # print('image set', image_set)

    # images = [image_name for img in image_set]
    image_len = len(image_set)
    np_images = [i.squeeze(0).detach().cpu().numpy() for i in image_set] # converting tensor images to numpy format
    axes=[]
    r,c=1,4 # r=1, c=4 with 4 classes

    mean = np.array([0.4])#, 0.456, 0.406])
    std = np.array([0.4])#, 0.224, 0.225])

    fig = plt.figure(figsize=(14,14))
    fig.subplots_adjust(hspace=0.01, wspace=0.01)
    # print('len of np_images:',len(np_images))
    for i in range(len(np_images)):
      ax = plt.subplot(r, c, i+1) 
      # ax.text(70, -4, image_name[i], fontsize=14, fontfamily='monospace')  

      plt.subplot(r, c, i+1) 
      plt.axis('off')
      # print('i=',i)
      # print('np_images',np_images[i])
      plt.imshow(np.clip(std*(np_images[i].transpose((0,1)))+mean, 0, 1))
      
  plt.show()

plot_trainsamples(_testsamples)

_testsamples[1]

import matplotlib.pyplot as plt
a = _trainsamples[28]
mask_image = a['mask']
mask_image.size()
# # pred_mask = a['pred_mask']
# upd = mask_image.squeeze(0).cpu()
# upd_np = upd.numpy()
# plt.imshow(upd_np[:,:])

import matplotlib.pyplot as plt
a = _trainsamples[28]
mask_image = a['mask']
pred_mask = a['pred_mask']
upd = pred_mask.squeeze(0).cpu()
upd_np = upd.detach().numpy()
plt.imshow(upd_np[:,:])

import matplotlib.pyplot as plt
a = _trainsamples[28]
mask_image = a['mask']
pred_mask = a['bgfg']
upd = pred_mask.squeeze(0).cpu()
upd_np = upd.detach().numpy()
plt.imshow(upd_np[:,:])

import matplotlib.pyplot as plt
a = _trainsamples[5]
mask_image = a['mask']
pred_mask = a['pred_dpmap']
upd = pred_mask.squeeze(0).cpu()
upd_np = upd.numpy()+0.6
plt.imshow(upd_np[:,:])



import matplotlib.pyplot as plt # lr 0.0001
plt.plot(iou_coeffs, label='IoU score')
# plt.plot(valid_losses, label='ssim index')
plt.legend()

import matplotlib.pyplot as plt

img = a['pred_mask']
# img.size()
upd = img.squeeze(0).cpu()
upd.size()

bgfg_np = upd
# bgfg_np
np_img = bgfg_np.numpy()
# type(np_img)
plt.imshow(np_img[:,:])

# print("bgfg size is: ", bgfg.size())
# from torchvision import transforms
# im = transforms.ToPILImage()(bgfg).convert("L")
# display(im)
# print(im)
# print(im.size)

from torch.utils.tensorboard import SummaryWriter
tb = SummaryWriter()

network = UNet(1,1)
images = enumerate(trainLoader)


input, target = images[1]
# print(labels)
# grid = torchvision.utils.make_grid(images)

# tb.add_image('images', grid)
# tb.add_graph(network, images)
# tb.close()

"""**UNET Testing**"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR,StepLR

use_cuda = torch.cuda.is_available()
device = torch.device('cuda' if use_cuda else 'cpu')  # set device to cuda

_samples = [] # to extract predictions

unet_model = UNet(1,1).to(device)
optimizer = optim.SGD(unet_model.parameters(),lr=0.001)
scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, verbose=True)
mask_criterion = DiceLoss()


unet = EvaluateModel(unet_model, trainLoader, device, testLoader, mask_criterion, optimizer, scheduler )
unet.train_test(epochs=30)

import matplotlib.pyplot as plt # lr 0.0001
plt.plot(unet.train.train_losses, label='train_loss')
plt.plot( unet.test.test_losses, label='test_loss')
plt.legend()

import matplotlib.pyplot as plt # this is for lr 0.001
plt.plot(unet.train.train_losses, label='train_loss')
plt.plot( unet.test.test_losses, label='test_loss')
plt.legend()

import matplotlib.pyplot as plt # this is for lr 0.0001
plt.plot(unet.train.train_losses, label='train_loss')
plt.plot( unet.test.test_losses, label='test_loss')
plt.legend()

train_a=train[1]
train_a

a=_samples[1]
a

bgfg= train_a['bgfg']

upd = bgfg.squeeze(0)
# upd.size()
bgfg_np = upd/2
# bgfg_np
np_img = bgfg_np.cpu().numpy()
type(np_img)
plt.imshow(np_img)

bgfg= train_a['mask']

upd = bgfg.squeeze(0)
# upd.size()
bgfg_np = upd/2

# bgfg_np
np_img = bgfg_np.cpu().numpy()
# type(np_img)
plt.imshow(np_img)

np_img.shape

from PIL import Image
pil_img = Image.fromarray(np_img, mode='L')
pil_img.save('n.png')

mask_t = _samples[1]['pred_mask']
mask = mask_t.cpu()
# type(bgfg_cpu)

print("mask size is: ", mask.size())
from torchvision import transforms
im = transforms.ToPILImage()(mask)#.convert("RGB")
display(im)
print(im)
print(im.size)

a=train[9]
bgfg=  _samples[1]['mask']
# type(bgfg)
a=bgfg.squeeze(0).cpu()
anp = a.numpy()
# type(anp)
img = Image.fromarray(anp, 'L')
img.save('out.png')

# import matplotlib.pyplot as plt
# plt.imshow(  a.permute(0,1)  )

img = Image.fromarray(a, 'RGB')
img.save('out.png')

"""**SAVING & LOADING MODEL**"""

path = ''

# SAVE
torch.save(unet_model, path)

# LOAD
model=torch.load(path)

"""**TENSORBOARD INTEGRATION**"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import torchvision
import torchvision.transforms as transforms
from torch.utils.tensorboard import SummaryWriter 

print(torch.__version__)
print(torchvision.__version__)


class Network(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)

    self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)
    self.fc2 = nn.Linear(in_features=120, out_features=60)
    self.out = nn.Linear(in_features=60, out_features=10)

  def forward(self, t):
    t = F.relu(self.conv1(t))
    t = F.max_pool2d(t, kernel_size=2, stride=2)

    t = F.relu(self.conv2(t))
    t = F.max_pool2d(t, kernel_size=2, stride=2)

    t = t.flatten(start_dim=1)
    t = F.relu(self.fc1(t))
    t = F.relu(self.fc2(t))
    t = self.out(t)

    return t

train_set = torchvision.datasets.FashionMNIST(
  root='./data',
  train=True,
  download=True,
  transform=transforms.Compose([
    transforms.ToTensor()
  ])
)
train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)

# Tensorboard Setting
 tb = SummaryWriter() # creating an object of summary writer
 network = Network()
 images, labels = next(iter(train_loader))
 grid = torchvision.utils.make_grid(images)

# add objects
# tb.add_images('images',grid)
tb.add_graph(network, images)
tb.close()

# Commented out IPython magic to ensure Python compatibility.
# python tensorboard_demo.py
from tensorflow import summary
# %load_ext tensorboard
# %tensorboard --logdir /content/runs

"""**TENSORBOARD COLAB**"""

! pip install tensorboardcolab

from tensorboardcolab import TensorBoardColab
tb = TensorBoardColab()

"""**SETTING MODEL CONFIGS**"""

class ModelConfig:
  def __init__(self, **kwargs):
    for k, v in kwargs:
      setattr(self, k, v)



model_config = Config(
    cuda = True if torch.cuda.is_available() else False,
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    seed = 9091,
    lr = 0.01,
    epochs = 4,
    save_model = False,
    batch_size = 32,
    log_interval = 100
)

import torch
import torchvision
from tqdm import notebook
import torch.nn.functional as F


class TrainModel:
    def __init__(self, model, device, trainloader, optimizer, scheduler, mask_criterion):
        self.model = model
        self.device = device
        self.trainloader = trainloader
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.mask_criterion = mask_criterion

        # initialize training variables
        self.train_losses = []
        self.train_acc = []
        self.learning_rate = []

        # pass time
        self.pass_time = []

        # TensorboardColab
        self.tb = TensorboardColab()
        self.train_iter = 0
        self.test_iter = 0

    def train(self):
        self.model.train()
        pbar = notebook.tqdm(enumerate(self.trainloader))
        mask_coeff=0
        start = time()
        for batch_idx, data in pbar:

          self.train_iter += 1
          bgfg, mask_target = data['bgfg'].to(self.device), data['mask'].to(self.device)
          
          # set optimzer to zero grad
          self.optimizer.zero_grad()
          
          # get the predictions
          pred_mask = self.model(bgfg)


          # calculate DICE LOSS
          mask_loss = self.mask_criterion(pred_mask, mask_target) 
          loss = mask_loss
          mask_coeff += dice_coefficient(pred_mask,mask_target, mask = True).item() # Segmentation Metric

          # BackProp
          loss.backward()
          self.optimizer.step()
          
          # Batch LR
          _lr = self.optimizer.param_groups[0]['lr']
          

          # Update PBAR-TQDM
          pbar.set_description(f'Loss={loss:0.3f}')
          end = time()
          total_time = end-start


          # if self.scheduler and isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
          #     self.scheduler.step(loss)

          
        print('Train set: Average loss: {:.4f}, Coef: ({:.5f})\n'.format((mask_loss),  (mask_coeff) /len(self.trainloader)))
        print('Train Set: Accuracy: {:.3f}'.format(mask_coeff/len(self.trainloader)))
        self.pass_time.append(total_time)
        self.train_losses.append((mask_loss))
        self.train_acc.append( mask_coeff/len(self.trainloader))
        self.learning_rate.append(_lr)
        
        # writing model attr to tensorboard
        self.tb.save_value('Train Accuracy', 'train acc', self.train_iter, mask_coeff/len(self.trainloader))
        self.tb.save_value('Train Loss', 'train loss', self.train_iter, mask_loss) 
        


class TestModel:
    def __init__(self, model, device, testloader, mask_criterion, scheduler):
      self.model = model
      self.device = device
      self.testloader = testloader
      self.mask_criterion = mask_criterion
      self.scheduler = scheduler
      self.test_losses = []
      self.test_acc = []
      # self.learning_rate = []
      

    def test(self):
        self.model.eval()
        mask_coeff=0
        with torch.no_grad():
          for batch_idx, data in enumerate(self.testloader):

            self.test_iter += 1
            bgfg, mask_target = data['bgfg'].to(self.device), data['mask'].to(self.device)          
            # Mask Predictions
            pred_mask = self.model(bgfg)
            # print('pred_mask type test', type(pred_mask))
            

            # Calculate DICE LOSS
            mask_loss = self.mask_criterion(pred_mask, mask_target) 
            loss = mask_loss
            mask_coeff += dice_coefficient(pred_mask,mask_target, mask = True).item() # Segmentation Metric
        
            # if self.scheduler and isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
            #     self.scheduler.step(loss)

            if batch_idx < 1:
              for i in range(self.testloader.batch_size):
                _samples.append({'pred_mask': pred_mask[i] ,'bgfg': bgfg[i]})


              
          print('Test Set: Average loss: {:.4f}, Coef: ({:.5f})\n'.format((mask_loss),  (mask_coeff) /len(self.testloader)))
          print('Test Set: Accuracy: {:.3f}'.format(mask_coeff/len(self.testloader)))
          self.test_losses.append((mask_loss))
          self.test_acc.append( mask_coeff/len(self.testloader))

          # writing test stats to tensorboard
          self.tb.save_value('Test Accuracy', 'test acc', self.test_iter, mask_coeff/len(self.testloader))
          self.tb.save_value('Test Loss', 'test loss', self.test_iter, mask_loss) 
          

class EvaluateModel:
    def __init__(self, model, trainloader, device, testloader, mask_criterion, optimizer, scheduler):
        self.model = model
        self.trainloader = trainloader
        self.testloader = testloader
        self.mask_criterion = mask_criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device

        self.train = TrainModel(self.model, self.device, self.trainloader, self.optimizer, self.scheduler, self.mask_criterion)
        self.test = TestModel(self.model, self.device,self.testloader, self.mask_criterion, self.scheduler)

    
    def train_test(self, epochs=10):
        pbar = notebook.tqdm(range(1, epochs+1), desc="Epochs")
        for epoch in pbar:
            # gc.collect()
            self.train.train()
            self.test.test()
            lr = self.optimizer.param_groups[0]['lr']     
            print('lr', lr)    
            if self.scheduler and not isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                self.scheduler.step()
            # pbar.write(f"Learning Rate = {lr:0.6f}")